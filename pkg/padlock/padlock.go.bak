// Copyright 2025 Ray Ozzie and his Mom. All rights reserved.

// Package padlock implements the high-level operations for encoding and decoding files
// using the K-of-N threshold one-time-pad cryptographic scheme.
//
// This package serves as the orchestration layer between:
// - The core cryptographic threshold scheme implementation (pkg/pad)
// - The file system operations layer (pkg/file)
// - The command-line interface (cmd/padlock)
//
// The padlock system provides information-theoretic security through:
// - A K-of-N threshold scheme: Any K out of N collections can reconstruct the data
// - One-time pad encryption: Uses truly random keys combined with XOR operations
// - Defense in depth: Multiple independent sources of randomness
// - Serialization: Processes entire directories with optional compression
//
// The key components of this package are:
//
// 1. EncodeDirectory: Splits an input directory into N collections
//   - Validates input/output directories
//   - Creates necessary directories and collections
//   - Serializes input directory to a tar stream
//   - Optionally compresses the data
//   - Processes chunks through the pad encoding
//   - Writes to collections in specified format
//   - Optionally creates ZIP archives for collections
//
// 2. DecodeDirectory: Reconstructs original data from K or more collections
//   - Locates and validates available collections
//   - Handles both directory and ZIP collection formats
//   - Sets up a pipeline for decoding and decompression
//   - Deserializes the decoded stream to output directory
//
// Security considerations:
// - Security depends entirely on the quality of randomness
// - Collections should be stored in separate locations
// - Same collections should never be reused for different data
package padlock

import (
	"context"
	"fmt"
	"io"
	"os"
	"path/filepath"
	"strings"
	"time"

	"github.com/blues/padlock/pkg/file"
	"github.com/blues/padlock/pkg/pad"
	"github.com/blues/padlock/pkg/trace"
)

// Format is a type alias for file.Format, representing the output format for collections.
// A Format determines how data chunks are written to and read from the filesystem.
type Format = file.Format

// Compression represents the compression mode used when serializing directories.
// This allows for space-efficient storage while maintaining the security properties
// of the threshold scheme.
type Compression int

const (
	// FormatBin is a binary format that stores data chunks directly as binary files.
	// This format is more efficient but less portable across different systems.
	FormatBin = file.FormatBin

	// FormatPNG is a PNG format that stores data chunks as images.
	// This format is useful for cases where binary files might be altered by
	// transfer systems, or where visual confirmation of collection existence is helpful.
	FormatPNG = file.FormatPNG

	// CompressionNone indicates no compression will be applied to the serialized data.
	// Use this when processing already compressed data or when processing speed is critical.
	CompressionNone Compression = iota

	// CompressionGzip indicates gzip compression will be applied to reduce storage requirements.
	// This is the default compression mode, providing good compression ratios with reasonable speed.
	CompressionGzip
)

// EncodeConfig holds configuration parameters for the encoding operation.
// This structure is created by the command-line interface and passed to EncodeDirectory.
type EncodeConfig struct {
	InputDir           string      // Path to the directory containing data to encode
	OutputDir          string      // Path where the encoded collections will be created (for backward compatibility)
	OutputDirs         []string    // List of output directories, one for each collection when multiple dirs are specified
	N                  int         // Total number of collections to create (N value)
	K                  int         // Minimum collections required for reconstruction (K value)
	Format             Format      // Output format (binary or PNG)
	ChunkSize          int         // Maximum size for data chunks in bytes
	RNG                pad.RNG     // Random number generator for one-time pad creation
	ClearIfNotEmpty    bool        // Whether to clear the output directory if not empty
	Verbose            bool        // Enable verbose logging
	Compression        Compression // Compression mode for the serialized data
	ArchiveCollections bool        // Whether to create TAR archives for collections
}

// DecodeConfig holds configuration parameters for the decoding operation.
// This structure is created by the command-line interface and passed to DecodeDirectory.
type DecodeConfig struct {
	InputDir        string      // Path to the directory containing collections to decode (for backward compatibility)
	InputDirs       []string    // List of input directories, each containing a collection to decode
	OutputDir       string      // Path where the decoded data will be written
	RNG             pad.RNG     // Random number generator (unused for decoding, but maintained for consistency)
	Verbose         bool        // Enable verbose logging
	Compression     Compression // Compression mode used when the data was encoded
	ClearIfNotEmpty bool        // Whether to clear the output directory if not empty
}

// EncodeDirectory encodes a directory using the padlock K-of-N threshold scheme.
//
// This function orchestrates the entire encoding process:
// 1. Validates the input and output directories
// 2. Creates the cryptographic pad with specified K-of-N parameters
// 3. Sets up the collection directories where encoded data will be written
// 4. Serializes the input directory to a tar stream
// 5. Optionally compresses the serialized data
// 6. Processes the data through the one-time pad encoder in chunks
// 7. Distributes encoded chunks across the collections
// 8. Optionally creates ZIP archives for easy distribution
//
// Parameters:
//   - ctx: Context with logging, cancellation, and tracing capabilities
//   - cfg: Configuration parameters for the encoding operation
//
// Returns:
//   - An error if any part of the encoding process fails, nil on success
//
// The encoding process ensures that the resulting collections have the following property:
// Any K or more collections can be used to reconstruct the original data, while
// K-1 or fewer collections reveal absolutely nothing about the original data.
func EncodeDirectory(ctx context.Context, cfg EncodeConfig) error {
	log := trace.FromContext(ctx).WithPrefix("PADLOCK")
	start := time.Now()
	
	// Log differently depending on whether using single or multiple output directories
	if len(cfg.OutputDirs) <= 1 {
		log.Infof("Starting encode: InputDir=%s OutputDir=%s", cfg.InputDir, cfg.OutputDir)
	} else {
		log.Infof("Starting encode: InputDir=%s with %d output directories", cfg.InputDir, len(cfg.OutputDirs))
		for i, dir := range cfg.OutputDirs {
			log.Debugf("  OutputDir[%d]=%s", i, dir)
		}
	}
	log.Debugf("Encode parameters: copies=%d, required=%d, Format=%s, ChunkSize=%d", cfg.N, cfg.K, cfg.Format, cfg.ChunkSize)

	// Validate input directory to ensure it exists and is accessible
	if err := file.ValidateInputDirectory(ctx, cfg.InputDir); err != nil {
		return err
	}

	// Prepare all output directories, clearing them if requested and they're not empty
	if len(cfg.OutputDirs) > 1 {
		// When using multiple output directories - prepare each one individually
		for _, dir := range cfg.OutputDirs {
			if err := file.PrepareOutputDirectory(ctx, dir, cfg.ClearIfNotEmpty); err != nil {
				return err
			}
		}
	} else {
		// Traditional single output directory approach
		if err := file.PrepareOutputDirectory(ctx, cfg.OutputDir, cfg.ClearIfNotEmpty); err != nil {
			return err
		}
	}

	// Create a new pad instance with the specified N and K parameters
	// This is the core cryptographic component that implements the threshold scheme
	log.Debugf("Creating pad instance with N=%d, K=%d", cfg.N, cfg.K)
	p, err := pad.NewPadForEncode(ctx, cfg.N, cfg.K)
	if err != nil {
		log.Error(fmt.Errorf("failed to create pad instance: %w", err))
		return err
	}

	// Create collections based on the configuration
	var collections []file.Collection
	if len(cfg.OutputDirs) > 1 {
		// Use multiple output directories - one collection per directory
		if len(cfg.OutputDirs) != len(p.Collections) {
			return fmt.Errorf("number of output directories (%d) does not match number of collections (%d)",
				len(cfg.OutputDirs), len(p.Collections))
		}
		
		// Create collections in individual directories
		collections = make([]file.Collection, len(p.Collections))
		for i, collName := range p.Collections {
			// For multiple output dirs, we use the actual directory as the collection directory
			collections[i] = file.Collection{
				Name: collName,
				Path: cfg.OutputDirs[i],
				Format: cfg.Format,
			}
			log.Debugf("Created collection %d: %s at %s", i+1, collName, cfg.OutputDirs[i])
		}
	} else if !cfg.ArchiveCollections {
		// For directory-based output, create collection subdirectories
		var err error
		collections, err = file.CreateCollections(ctx, cfg.OutputDir, p.Collections)
		if err != nil {
			return err
		}
		
		// Set format for all collections
		for i := range collections {
			collections[i].Format = cfg.Format
		}
	} else {
		// For TAR-based output in a single directory, just create collection references
		// without actually creating directories (we'll write directly to TAR files)
		collections = make([]file.Collection, len(p.Collections))
		for i, collName := range p.Collections {
			collections[i] = file.Collection{
				Name: collName,
				Path: filepath.Join(cfg.OutputDir, collName),
				Format: cfg.Format,
			}
			log.Debugf("Created virtual collection %d: %s at %s", i+1, collName, collections[i].Path)
		}
	}

	// Get the formatter for the specified format (binary or PNG)
	// This determines how data chunks are written to and read from disk
	formatter := file.GetFormatter(cfg.Format)

	// Create a tar stream from the input directory
	// This serializes all files and directories into a single stream for processing
	log.Debugf("Creating tar stream from input directory: %s", cfg.InputDir)
	tarStream, err := file.SerializeDirectoryToStream(ctx, cfg.InputDir)
	if err != nil {
		log.Error(fmt.Errorf("failed to create tar stream: %w", err))
		return fmt.Errorf("failed to create tar stream: %w", err)
	}
	defer tarStream.Close()

	// Add compression if configured (typically GZIP)
	// This reduces storage requirements without affecting security
	var inputStream io.Reader = tarStream
	if cfg.Compression == CompressionGzip {
		log.Debugf("Adding gzip compression to stream")
		inputStream = file.CompressStreamToStream(ctx, tarStream)
	}

	// Define a callback function that creates chunk writers for the encoding process
	// Each time the pad encoder needs to write a chunk, this function is called
	//
	// When zip collections is enabled, this will create ZipChunkWriters to write
	// chunks directly to ZIP files instead of temporary files on disk.
	newChunkFunc := func(collectionName string, chunkNumber int, chunkFormat string) (io.WriteCloser, error) {
		// Find the collection path for the given collection name
		var collPath string
		var found bool
		
		for _, c := range collections {
			if c.Name == collectionName {
				collPath = c.Path
				found = true
				break
			}
		}

		if !found || collPath == "" {
			return nil, fmt.Errorf("collection not found: %s", collectionName)
		}
		
		// If archive collections is enabled, create TarChunkWriter
		if cfg.ArchiveCollections {
			// For direct TAR output, create or reuse a TarChunkWriter
			// The tar path will be the collection path with .tar extension
			tarPath := collPath
			if !strings.HasSuffix(tarPath, ".tar") {
				tarPath = tarPath + ".tar"
			}
			
			// Create the TarChunkWriter for this chunk if it doesn't exist yet
			tarWriter, err := file.NewTarChunkWriter(ctx, tarPath, collectionName, cfg.Format)
			if err != nil {
				return nil, fmt.Errorf("failed to create tar chunk writer: %w", err)
			}
			
			// Set the chunk number for this write operation
			tarWriter.ChunkNum = chunkNumber
			
			return tarWriter, nil
		}
		
		// Otherwise use the standard NamedChunkWriter for directory output
		return &file.NamedChunkWriter{
			Ctx:       ctx,
			Formatter: formatter,
			CollPath:  collPath,
			CollName:  collectionName,
			ChunkNum:  chunkNumber,
		}, nil
	}

	// Run the actual encoding process, which:
	// 1. Reads data from the input stream in chunks
	// 2. Generates random one-time pads for each chunk
	// 3. XORs input data with pads to create ciphertext
	// 4. Distributes the results across collections according to the threshold scheme
	log.Debugf("Starting encode process with chunk size: %d", cfg.ChunkSize)
	err = p.Encode(
		ctx,
		cfg.ChunkSize,
		inputStream,
		cfg.RNG,
		newChunkFunc,
		string(cfg.Format),
	)
	if err != nil {
		log.Error(fmt.Errorf("encoding failed: %w", err))
		return fmt.Errorf("encoding failed: %w", err)
	}

	// If archives were enabled, the chunks have already been written directly to TAR files
	// We need to finalize the TAR writers to ensure they're properly closed
	if cfg.ArchiveCollections {
		// Finalize all TAR writers to ensure proper closing
		log.Debugf("Finalizing all TAR writers created during encoding")
		if err := file.FinalizeAllTarWriters(ctx); err != nil {
			log.Error(fmt.Errorf("failed to finalize TAR writers: %w", err))
			return err
		}
		log.Debugf("All TAR writers finalized successfully")
		
		// Clean up the empty collection directories that were created
		// since we're using direct TAR creation instead of directories
		log.Debugf("Cleaning up empty collection directories after creating TAR files")
		for _, coll := range collections {
			// Only remove if it's a directory and not a TAR file
			if !strings.HasSuffix(coll.Path, ".tar") {
				info, err := os.Stat(coll.Path)
				if err == nil && info.IsDir() {
					if err := os.RemoveAll(coll.Path); err != nil {
						log.Debugf("Warning: Failed to remove collection directory: %s (%v)", coll.Path, err)
					} else {
						log.Debugf("Removed collection directory: %s", coll.Path)
					}
				}
			}
		}
	} else if len(cfg.OutputDirs) > 1 {
		// For multiple output directories, create tar archives within each directory
		// but don't delete the directories (just archive the contents)
		for _, coll := range collections {
			tarPath, err := file.TarDirectoryContents(ctx, coll.Path, coll.Name)
			if err != nil {
				log.Error(fmt.Errorf("failed to create tar archive for collection %s: %w", coll.Name, err))
				return err
			}
			log.Infof("Created tar archive for collection %s: %s", coll.Name, tarPath)
		}
	} else {
		// Traditional approach - delete the directories after creating archives
		if _, err := file.TarCollections(ctx, collections); err != nil {
			return err
		}
	}

	// Log completion information including elapsed time
	elapsed := time.Since(start)
	
	// Log differently depending on whether using single or multiple output directories
	if len(cfg.OutputDirs) <= 1 {
		log.Infof("Encode complete (%s) -copies %d -required %d -format %s", elapsed, cfg.N, cfg.K, cfg.Format)
	} else {
		log.Infof("Encode complete (%s) with %d output directories -required %d -format %s", 
			elapsed, len(cfg.OutputDirs), cfg.K, cfg.Format)
	}
	
	return nil
}

// isValidCollectionDir checks if a directory is likely to contain a valid collection
func isValidCollectionDir(ctx context.Context, dirPath string) bool {
	log := trace.FromContext(ctx).WithPrefix("PADLOCK")
	log.Debugf("Checking if %s is a valid collection directory", dirPath)
	
	// Try to determine collection format
	format, err := file.DetermineCollectionFormat(dirPath)
	if err != nil {
		log.Debugf("%s is not a valid collection directory: %v", dirPath, err)
		return false
	}
	
	log.Debugf("%s appears to be a valid collection directory with format %s", dirPath, format)
	return true
}

// determineCollectionNameFromContent tries to deduce the collection name by examining files
func determineCollectionNameFromContent(ctx context.Context, dirPath string) (string, error) {
	log := trace.FromContext(ctx).WithPrefix("PADLOCK")
	
	// Read the directory
	entries, err := os.ReadDir(dirPath)
	if err != nil {
		return "", fmt.Errorf("failed to read directory: %w", err)
	}
	
	// Look for files with pattern like "IMG3A5_0001.PNG" or "3A5_0001.bin"
	for _, entry := range entries {
		if entry.IsDir() {
			continue
		}
		
		name := entry.Name()
		
		// Check for PNG files
		if strings.HasSuffix(strings.ToUpper(name), ".PNG") && strings.HasPrefix(name, "IMG") {
			// Extract the collection name after "IMG" and before "_"
			parts := strings.Split(strings.TrimPrefix(name, "IMG"), "_")
			if len(parts) > 0 && file.IsCollectionName(parts[0]) {
				log.Debugf("Determined collection name '%s' from file %s", parts[0], name)
				return parts[0], nil
			}
		}
		
		// Check for bin files
		if strings.HasSuffix(name, ".bin") {
			// Extract the collection name before "_"
			parts := strings.Split(name, "_")
			if len(parts) > 0 && file.IsCollectionName(parts[0]) {
				log.Debugf("Determined collection name '%s' from file %s", parts[0], name)
				return parts[0], nil
			}
		}
	}
	
	return "", fmt.Errorf("could not determine collection name from directory content")
}

// DecodeDirectory reconstructs original data from K or more collections using the padlock scheme.
//
// This function orchestrates the entire decoding process:
// 1. Validates the input and output directories
// 2. Locates and loads available collections (from directories or ZIP files)
// 3. Creates readers for each collection to access the encoded chunks
// 4. Sets up a parallel deserialization pipeline using goroutines
// 5. Creates the pad instance for decoding based on available collections
// 6. Processes the collections through the one-time pad decoder
// 7. Deserializes the decoded data to the output directory
//
// Parameters:
//   - ctx: Context with logging, cancellation, and tracing capabilities
//   - cfg: Configuration parameters for the decoding operation
//
// Returns:
//   - An error if any part of the decoding process fails, nil on success
//
// The decoding process can succeed only if at least K collections from the original
// N collections are provided. With fewer than K collections, the function will fail
// and no information about the original data can be recovered due to the information-theoretic
// security properties of the threshold scheme.
func DecodeDirectory(ctx context.Context, cfg DecodeConfig) error {
	log := trace.FromContext(ctx).WithPrefix("PADLOCK")
	start := time.Now()
	
	// Log differently depending on whether using single or multiple input directories
	if len(cfg.InputDirs) <= 1 {
		log.Infof("Starting decode: InputDir=%s OutputDir=%s", cfg.InputDir, cfg.OutputDir)
	} else {
		log.Infof("Starting decode with %d input directories, OutputDir=%s", len(cfg.InputDirs), cfg.OutputDir)
		for i, dir := range cfg.InputDirs {
			log.Debugf("  InputDir[%d]=%s", i, dir)
		}
	}

	// Prepare the output directory, clearing it if requested and it's not empty
	if err := file.PrepareOutputDirectory(ctx, cfg.OutputDir, cfg.ClearIfNotEmpty); err != nil {
		return err
	}

	// Variable to hold all collected collections and a tempDir if needed
	var allCollections []file.Collection
	var collTempDir string
	
	// Handle single input dir or multiple input dirs
	if len(cfg.InputDirs) <= 1 {
		// Traditional approach - single input directory containing multiple collections
		// Validate input directory to ensure it exists and is accessible
		if err := file.ValidateInputDirectory(ctx, cfg.InputDir); err != nil {
			return err
		}
		
		// Find collections (directories or zips) in the input directory
		// This identifies all available collections, extracting ZIP files if necessary
		collections, tempDir, err := file.FindCollections(ctx, cfg.InputDir)
		if err != nil {
			return err
		}
		
		// Use the results
		allCollections = collections
		collTempDir = tempDir
	} else {
		// Multiple input directory mode - each input directory is treated as a collection
		for _, inputDir := range cfg.InputDirs {
			// Validate each input directory
			if err := file.ValidateInputDirectory(ctx, inputDir); err != nil {
				return err
			}
			
			// First check if this directory contains a collection directly
			// (it might be a directory containing a collection like '3A5')
			if isValidCollectionDir(ctx, inputDir) {
				// The directory itself is a valid collection
				format, err := file.DetermineCollectionFormat(inputDir)
				if err != nil {
					log.Infof("Could not determine collection format for %s, skipping: %v", inputDir, err)
					continue
				}
				
				collName := filepath.Base(inputDir)
				if !file.IsCollectionName(collName) {
					// If the directory name is not a valid collection name,
					// try to find a valid collection inside by examining files
					collName, err = determineCollectionNameFromContent(ctx, inputDir)
					if err != nil {
						log.Infof("Could not determine collection name for %s, skipping: %v", inputDir, err)
						continue
					}
				}
				
				collection := file.Collection{
					Name:   collName,
					Path:   inputDir,
					Format: format,
				}
				allCollections = append(allCollections, collection)
				log.Debugf("Found direct collection in %s, name=%s, format=%s", inputDir, collName, format)
			} else {
				// Check if the directory contains collections or zip files
				collections, tempDir, err := file.FindCollections(ctx, inputDir)
				if err != nil {
					log.Infof("Failed to find collections in %s: %v", inputDir, err)
					continue
				}
				
				// Add these collections to our master list
				allCollections = append(allCollections, collections...)
				
				// Remember the tempDir for cleanup if it exists
				if tempDir != "" && collTempDir == "" {
					collTempDir = tempDir
				}
				
				log.Debugf("Found %d collections in directory %s", len(collections), inputDir)
			}
		}
	}

	// If we extracted zip files, clean up the temporary directory when done
	if collTempDir != "" {
		defer func() {
			log.Debugf("Cleaning up temporary directory: %s", collTempDir)
			os.RemoveAll(collTempDir)
		}()
	}

	// Ensure we found at least some collections
	if len(allCollections) == 0 {
		if len(cfg.InputDirs) <= 1 {
			log.Error(fmt.Errorf("no collections found in input directory"))
			return fmt.Errorf("no collections found in input directory")
		} else {
			log.Error(fmt.Errorf("no valid collections found in any of the input directories"))
			return fmt.Errorf("no valid collections found in any of the input directories")
		}
	}
	log.Debugf("Found total of %d collections", len(allCollections))

	// Create readers for each collection
	// These readers handle the format-specific details of reading chunks
	readers := make([]io.Reader, len(allCollections))
	collReaders := make([]*file.CollectionReader, len(allCollections))

	for i, coll := range allCollections {
		// Create a collection reader
		collReader := file.NewCollectionReader(coll)
		collReaders[i] = collReader
		
		// Create an adapter that converts the CollectionReader to an io.Reader
		chunkReader := file.NewChunkReaderAdapter(ctx, collReader)
		
		// Store the reader
		readers[i] = chunkReader
	}

	// Get the number of available collections (important for pad initialization)
	n := len(allCollections)
	log.Infof("Collections: %d", n)

	// Create a pipe for transferring decoded data between goroutines
	// This allows parallel processing of decoding and deserialization
	log.Debugf("Creating pipe for decoded data")
	pr, pw := io.Pipe()

	// Channel to signal completion of the deserialization goroutine
	done := make(chan struct{})

	// Start the deserialization process in a separate goroutine
	// This goroutine reads from the pipe and writes to the output directory
	var deserializeErr error
	go func() {
		defer close(done) // Signal completion via the done channel
		defer pr.Close()  // Ensure pipe reader is closed when this goroutine exits

		deserializeCtx := trace.WithContext(ctx, log.WithPrefix("DESERIALIZE"))

		// Create decompression stream if needed
		// This reverses any compression applied during encoding
		var outputStream io.Reader = pr
		if cfg.Compression == CompressionGzip {
			log.Debugf("Creating decompression stream")
			var err error
			outputStream, err = file.DecompressStreamToStream(deserializeCtx, pr)
			if err != nil {
				log.Error(fmt.Errorf("failed to create decompression stream: %w", err))
				deserializeErr = err
				return
			}
		}

		// Deserialize the tar stream to the output directory
		// This reconstructs the original directory structure and files
		log.Debugf("Deserializing to output directory: %s", cfg.OutputDir)
		err := file.DeserializeDirectoryFromStream(deserializeCtx, cfg.OutputDir, outputStream, cfg.ClearIfNotEmpty)
		if err != nil {
			// Special case: Don't treat "too small" tar file as an error for small inputs
			if strings.Contains(err.Error(), "too small to be a valid tar file") {
				log.Infof("Input data appears to be a small raw file rather than a tar archive")
				deserializeErr = nil
			} else {
				log.Error(fmt.Errorf("failed to deserialize directory: %w", err))
				deserializeErr = err
			}
		}
	}()

	// Create a new pad instance for decoding
	// The pad is initialized with the number of available collections
	// The K value will be extracted from the collection metadata during decoding
	log.Debugf("Creating pad instance with N=%d", n)
	p, err := pad.NewPadForDecode(ctx, n)
	if err != nil {
		log.Error(fmt.Errorf("failed to create pad instance: %w", err))
		return err
	}

	// Run the decoding process
	log.Debugf("Starting decode process")

	// Create collection names list for logging purposes
	collectionNames := make([]string, len(allCollections))
	for i, coll := range allCollections {
		collectionNames[i] = coll.Name
	}

	// Decode the collections
	// This combines the chunks from different collections using the threshold scheme
	// The result is written to the pipe writer (pw)
	err = p.Decode(ctx, readers, pw)
	if err != nil {
		log.Error(fmt.Errorf("decoding failed: %w", err))
		return fmt.Errorf("decoding failed: %w", err)
	}

	// Close the pipe writer to signal the end of data to the deserialization goroutine
	err = pw.Close()
	if err != nil {
		log.Error(fmt.Errorf("error closing pipe writer: %w", err))
		// Continue anyway, as the pipe might already be closed by the deserialization goroutine
	}

	// In test environment, wait with a shorter timeout, but in production wait with a longer timeout
	timeoutDuration := 30 * time.Second
	if os.Getenv("GO_TEST") != "" || (ctx.Value(trace.TracerKey{}) != nil && strings.Contains(ctx.Value(trace.TracerKey{}).(*trace.Tracer).GetPrefix(), "TEST")) {
		timeoutDuration = 3 * time.Second
	}

	select {
	case <-done:
		log.Debugf("Deserialization goroutine completed")
	case <-time.After(timeoutDuration):
		// Avoid panic on pipe error
		pw.CloseWithError(fmt.Errorf("timeout waiting for deserialization to complete"))
		log.Error(fmt.Errorf("timeout waiting for deserialization to complete after %v", timeoutDuration))
		return fmt.Errorf("timeout waiting for deserialization to complete after %v", timeoutDuration)
	}

	// Check if there was an error in the deserialization
	if deserializeErr != nil {
		return deserializeErr
	}

	// No special cleanup needed for collection readers

	// Log completion information including elapsed time
	elapsed := time.Since(start)
	log.Infof("Decode complete (%s)", elapsed)
	return nil
}